% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.3 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated as
% required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup

\datalist[entry]{nty/global//global/global/global}
  \entry{darcet2024visiontransformersneedregisters}{misc}{}{}
    \name{author}{4}{}{%
      {{hash=DT}{%
         family={Darcet},
         familyi={D\bibinitperiod},
         given={Timothée},
         giveni={T\bibinitperiod},
      }}%
      {{hash=OM}{%
         family={Oquab},
         familyi={O\bibinitperiod},
         given={Maxime},
         giveni={M\bibinitperiod},
      }}%
      {{hash=MJ}{%
         family={Mairal},
         familyi={M\bibinitperiod},
         given={Julien},
         giveni={J\bibinitperiod},
      }}%
      {{hash=BP}{%
         family={Bojanowski},
         familyi={B\bibinitperiod},
         given={Piotr},
         giveni={P\bibinitperiod},
      }}%
    }
    \strng{namehash}{DT+1}
    \strng{fullhash}{DTOMMJBP1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{D}
    \field{sortinithash}{D}
    \verb{eprint}
    \verb 2309.16588
    \endverb
    \field{title}{Vision Transformers Need Registers}
    \verb{url}
    \verb https://arxiv.org/abs/2309.16588
    \endverb
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.CV}
    \field{year}{2024}
  \endentry

  \entry{visiontransformers2021}{misc}{}{}
    \name{author}{12}{}{%
      {{hash=DA}{%
         family={Dosovitskiy},
         familyi={D\bibinitperiod},
         given={Alexey},
         giveni={A\bibinitperiod},
      }}%
      {{hash=BL}{%
         family={Beyer},
         familyi={B\bibinitperiod},
         given={Lucas},
         giveni={L\bibinitperiod},
      }}%
      {{hash=KA}{%
         family={Kolesnikov},
         familyi={K\bibinitperiod},
         given={Alexander},
         giveni={A\bibinitperiod},
      }}%
      {{hash=WD}{%
         family={Weissenborn},
         familyi={W\bibinitperiod},
         given={Dirk},
         giveni={D\bibinitperiod},
      }}%
      {{hash=ZX}{%
         family={Zhai},
         familyi={Z\bibinitperiod},
         given={Xiaohua},
         giveni={X\bibinitperiod},
      }}%
      {{hash=UT}{%
         family={Unterthiner},
         familyi={U\bibinitperiod},
         given={Thomas},
         giveni={T\bibinitperiod},
      }}%
      {{hash=DM}{%
         family={Dehghani},
         familyi={D\bibinitperiod},
         given={Mostafa},
         giveni={M\bibinitperiod},
      }}%
      {{hash=MM}{%
         family={Minderer},
         familyi={M\bibinitperiod},
         given={Matthias},
         giveni={M\bibinitperiod},
      }}%
      {{hash=HG}{%
         family={Heigold},
         familyi={H\bibinitperiod},
         given={Georg},
         giveni={G\bibinitperiod},
      }}%
      {{hash=GS}{%
         family={Gelly},
         familyi={G\bibinitperiod},
         given={Sylvain},
         giveni={S\bibinitperiod},
      }}%
      {{hash=UJ}{%
         family={Uszkoreit},
         familyi={U\bibinitperiod},
         given={Jakob},
         giveni={J\bibinitperiod},
      }}%
      {{hash=HN}{%
         family={Houlsby},
         familyi={H\bibinitperiod},
         given={Neil},
         giveni={N\bibinitperiod},
      }}%
    }
    \strng{namehash}{DA+1}
    \strng{fullhash}{DABLKAWDZXUTDMMMHGGSUJHN1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{D}
    \field{sortinithash}{D}
    \verb{eprint}
    \verb 2010.11929
    \endverb
    \field{title}{An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}
    \verb{url}
    \verb https://arxiv.org/abs/2010.11929
    \endverb
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.CV}
    \field{year}{2021}
  \endentry

  \entry{10.1145/3586074}{article}{}{}
    \name{author}{3}{}{%
      {{hash=FQ}{%
         family={Fournier},
         familyi={F\bibinitperiod},
         given={Quentin},
         giveni={Q\bibinitperiod},
      }}%
      {{hash=CGM}{%
         family={Caron},
         familyi={C\bibinitperiod},
         given={Ga\'{e}tan\bibnamedelima Marceau},
         giveni={G\bibinitperiod\bibinitdelim M\bibinitperiod},
      }}%
      {{hash=AD}{%
         family={Aloise},
         familyi={A\bibinitperiod},
         given={Daniel},
         giveni={D\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Association for Computing Machinery}%
    }
    \keyw{survey, self-attention, efficient transformer, Deep learning}
    \strng{namehash}{FQCGMAD1}
    \strng{fullhash}{FQCGMAD1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{F}
    \field{sortinithash}{F}
    \field{abstract}{%
    Recurrent neural networks are effective models to process sequences. However, they are unable to learn long-term dependencies because of their inherent sequential nature. As a solution, Vaswani et al. introduced the Transformer, a model solely based on the attention mechanism that is able to relate any two positions of the input sequence, hence modelling arbitrary long dependencies. The Transformer has improved the state-of-the-art across numerous sequence modelling tasks. However, its effectiveness comes at the expense of a quadratic computational and memory complexity with respect to the sequence length, hindering its adoption. Fortunately, the deep learning community has always been interested in improving the models’ efficiency, leading to a plethora of solutions such as parameter sharing, pruning, mixed-precision, and knowledge distillation. Recently, researchers have directly addressed the Transformer’s limitation by designing lower-complexity alternatives such as the Longformer, Reformer, Linformer, and Performer. However, due to the wide range of solutions, it has become challenging for researchers and practitioners to determine which methods to apply in practice to meet the desired tradeoff between capacity, computation, and memory. This survey addresses this issue by investigating popular approaches to make Transformers faster and lighter and by providing a comprehensive explanation of the methods’ strengths, limitations, and underlying assumptions.%
    }
    \verb{doi}
    \verb 10.1145/3586074
    \endverb
    \field{issn}{0360-0300}
    \field{number}{14s}
    \field{title}{A Practical Survey on Faster and Lighter Transformers}
    \verb{url}
    \verb https://doi.org/10.1145/3586074
    \endverb
    \field{volume}{55}
    \list{location}{1}{%
      {New York, NY, USA}%
    }
    \field{journaltitle}{ACM Comput. Surv.}
    \field{month}{07}
    \field{year}{2023}
  \endentry

  \entry{10.1145/3505244}{article}{}{}
    \name{author}{6}{}{%
      {{hash=KS}{%
         family={Khan},
         familyi={K\bibinitperiod},
         given={Salman},
         giveni={S\bibinitperiod},
      }}%
      {{hash=NM}{%
         family={Naseer},
         familyi={N\bibinitperiod},
         given={Muzammal},
         giveni={M\bibinitperiod},
      }}%
      {{hash=HM}{%
         family={Hayat},
         familyi={H\bibinitperiod},
         given={Munawar},
         giveni={M\bibinitperiod},
      }}%
      {{hash=ZSW}{%
         family={Zamir},
         familyi={Z\bibinitperiod},
         given={Syed\bibnamedelima Waqas},
         giveni={S\bibinitperiod\bibinitdelim W\bibinitperiod},
      }}%
      {{hash=KFS}{%
         family={Khan},
         familyi={K\bibinitperiod},
         given={Fahad\bibnamedelima Shahbaz},
         giveni={F\bibinitperiod\bibinitdelim S\bibinitperiod},
      }}%
      {{hash=SM}{%
         family={Shah},
         familyi={S\bibinitperiod},
         given={Mubarak},
         giveni={M\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Association for Computing Machinery}%
    }
    \keyw{Self-attention, transformers, bidirectional encoders, deep neural networks, convolutional networks, self-supervision, literature survey}
    \strng{namehash}{KS+1}
    \strng{fullhash}{KSNMHMZSWKFSSM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{K}
    \field{sortinithash}{K}
    \field{abstract}{%
    Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks, e.g., Long short-term memory. Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text, and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers, i.e., self-attention, large-scale pre-training, and bidirectional feature encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization), and three-dimensional analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works. We hope this effort will ignite further interest in the community to solve current challenges toward the application of transformer models in computer vision.%
    }
    \verb{doi}
    \verb 10.1145/3505244
    \endverb
    \field{issn}{0360-0300}
    \field{number}{10s}
    \field{title}{Transformers in Vision: A Survey}
    \verb{url}
    \verb https://doi.org/10.1145/3505244
    \endverb
    \field{volume}{54}
    \list{location}{1}{%
      {New York, NY, USA}%
    }
    \field{journaltitle}{ACM Comput. Surv.}
    \field{month}{09}
    \field{year}{2022}
  \endentry

  \entry{Liu2024-lm}{article}{}{}
    \name{author}{6}{}{%
      {{hash=LY}{%
         family={Liu},
         familyi={L\bibinitperiod},
         given={Yun},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=WYH}{%
         family={Wu},
         familyi={W\bibinitperiod},
         given={Yu-Huan},
         giveni={Y\bibinithyphendelim H\bibinitperiod},
      }}%
      {{hash=SG}{%
         family={Sun},
         familyi={S\bibinitperiod},
         given={Guolei},
         giveni={G\bibinitperiod},
      }}%
      {{hash=ZL}{%
         family={Zhang},
         familyi={Z\bibinitperiod},
         given={Le},
         giveni={L\bibinitperiod},
      }}%
      {{hash=CA}{%
         family={Chhatkuli},
         familyi={C\bibinitperiod},
         given={Ajad},
         giveni={A\bibinitperiod},
      }}%
      {{hash=VGL}{%
         family={Van\bibnamedelima Gool},
         familyi={V\bibinitperiod\bibinitdelim G\bibinitperiod},
         given={Luc},
         giveni={L\bibinitperiod},
      }}%
    }
    \list{language}{1}{%
      {en}%
    }
    \strng{namehash}{LY+1}
    \strng{fullhash}{LYWYHSGZLCAVGL1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{L}
    \field{sortinithash}{L}
    \field{number}{4}
    \field{pages}{670\bibrangedash 683}
    \field{title}{Vision transformers with hierarchical attention}
    \field{volume}{21}
    \field{journaltitle}{Mach. Intell. Res.}
    \field{year}{2024}
  \endentry

  \entry{vit-state-challenges}{misc}{}{}
    \name{author}{3}{}{%
      {{hash=RBK}{%
         family={Ruan},
         familyi={R\bibinitperiod},
         given={Bo-Kai},
         giveni={B\bibinithyphendelim K\bibinitperiod},
      }}%
      {{hash=SHH}{%
         family={Shuai},
         familyi={S\bibinitperiod},
         given={Hong-Han},
         giveni={H\bibinithyphendelim H\bibinitperiod},
      }}%
      {{hash=CWH}{%
         family={Cheng},
         familyi={C\bibinitperiod},
         given={Wen-Huang},
         giveni={W\bibinithyphendelim H\bibinitperiod},
      }}%
    }
    \strng{namehash}{RBKSHHCWH1}
    \strng{fullhash}{RBKSHHCWH1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{R}
    \field{sortinithash}{R}
    \verb{eprint}
    \verb 2207.03041
    \endverb
    \field{title}{Vision Transformers: State of the Art and Research Challenges}
    \verb{url}
    \verb https://arxiv.org/abs/2207.03041
    \endverb
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.CV}
    \field{year}{2022}
  \endentry

  \entry{ryoo2022tokenlearner8learnedtokens}{misc}{}{}
    \name{author}{5}{}{%
      {{hash=RMS}{%
         family={Ryoo},
         familyi={R\bibinitperiod},
         given={Michael\bibnamedelima S.},
         giveni={M\bibinitperiod\bibinitdelim S\bibinitperiod},
      }}%
      {{hash=PA}{%
         family={Piergiovanni},
         familyi={P\bibinitperiod},
         given={AJ},
         giveni={A\bibinitperiod},
      }}%
      {{hash=AA}{%
         family={Arnab},
         familyi={A\bibinitperiod},
         given={Anurag},
         giveni={A\bibinitperiod},
      }}%
      {{hash=DM}{%
         family={Dehghani},
         familyi={D\bibinitperiod},
         given={Mostafa},
         giveni={M\bibinitperiod},
      }}%
      {{hash=AA}{%
         family={Angelova},
         familyi={A\bibinitperiod},
         given={Anelia},
         giveni={A\bibinitperiod},
      }}%
    }
    \strng{namehash}{RMS+1}
    \strng{fullhash}{RMSPAAADMAA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{R}
    \field{sortinithash}{R}
    \verb{eprint}
    \verb 2106.11297
    \endverb
    \field{title}{TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?}
    \verb{url}
    \verb https://arxiv.org/abs/2106.11297
    \endverb
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.CV}
    \field{year}{2022}
  \endentry

  \entry{10.1007/978-3-031-20053-3_30}{inproceedings}{}{}
    \name{author}{3}{}{%
      {{hash=TH}{%
         family={Touvron},
         familyi={T\bibinitperiod},
         given={Hugo},
         giveni={H\bibinitperiod},
      }}%
      {{hash=CM}{%
         family={Cord},
         familyi={C\bibinitperiod},
         given={Matthieu},
         giveni={M\bibinitperiod},
      }}%
      {{hash=JH}{%
         family={J{\'e}gou},
         familyi={J\bibinitperiod},
         given={Herv{\'e}},
         giveni={H\bibinitperiod},
      }}%
    }
    \name{editor}{5}{}{%
      {{hash=AS}{%
         family={Avidan},
         familyi={A\bibinitperiod},
         given={Shai},
         giveni={S\bibinitperiod},
      }}%
      {{hash=BG}{%
         family={Brostow},
         familyi={B\bibinitperiod},
         given={Gabriel},
         giveni={G\bibinitperiod},
      }}%
      {{hash=CM}{%
         family={Ciss{\'e}},
         familyi={C\bibinitperiod},
         given={Moustapha},
         giveni={M\bibinitperiod},
      }}%
      {{hash=FGM}{%
         family={Farinella},
         familyi={F\bibinitperiod},
         given={Giovanni\bibnamedelima Maria},
         giveni={G\bibinitperiod\bibinitdelim M\bibinitperiod},
      }}%
      {{hash=HT}{%
         family={Hassner},
         familyi={H\bibinitperiod},
         given={Tal},
         giveni={T\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer Nature Switzerland}%
    }
    \strng{namehash}{THCMJH1}
    \strng{fullhash}{THCMJH1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{T}
    \field{sortinithash}{T}
    \field{abstract}{%
    A Vision Transformer (ViT) is a simple neural architecture amenable to serve several computer vision tasks. It has limited built-in architectural priors, in contrast to more recent architectures that incorporate priors either about the input data or of specific tasks. Recent works show that ViTs benefit from self-supervised pre-training, in particular BerT-like pre-training like BeiT.%
    }
    \field{booktitle}{Computer Vision -- ECCV 2022}
    \field{isbn}{978-3-031-20053-3}
    \field{pages}{516\bibrangedash 533}
    \field{title}{DeiT III: Revenge of the ViT}
    \list{location}{1}{%
      {Cham}%
    }
    \field{year}{2022}
  \endentry

  \entry{transformer2017}{misc}{}{}
    \name{author}{8}{}{%
      {{hash=VA}{%
         family={Vaswani},
         familyi={V\bibinitperiod},
         given={Ashish},
         giveni={A\bibinitperiod},
      }}%
      {{hash=SN}{%
         family={Shazeer},
         familyi={S\bibinitperiod},
         given={Noam},
         giveni={N\bibinitperiod},
      }}%
      {{hash=PN}{%
         family={Parmar},
         familyi={P\bibinitperiod},
         given={Niki},
         giveni={N\bibinitperiod},
      }}%
      {{hash=UJ}{%
         family={Uszkoreit},
         familyi={U\bibinitperiod},
         given={Jakob},
         giveni={J\bibinitperiod},
      }}%
      {{hash=JL}{%
         family={Jones},
         familyi={J\bibinitperiod},
         given={Llion},
         giveni={L\bibinitperiod},
      }}%
      {{hash=GAN}{%
         family={Gomez},
         familyi={G\bibinitperiod},
         given={Aidan\bibnamedelima N.},
         giveni={A\bibinitperiod\bibinitdelim N\bibinitperiod},
      }}%
      {{hash=KL}{%
         family={Kaiser},
         familyi={K\bibinitperiod},
         given={Lukasz},
         giveni={L\bibinitperiod},
      }}%
      {{hash=PI}{%
         family={Polosukhin},
         familyi={P\bibinitperiod},
         given={Illia},
         giveni={I\bibinitperiod},
      }}%
    }
    \strng{namehash}{VA+1}
    \strng{fullhash}{VASNPNUJJLGANKLPI1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{V}
    \field{sortinithash}{V}
    \verb{eprint}
    \verb 1706.03762
    \endverb
    \field{title}{Attention Is All You Need}
    \verb{url}
    \verb https://arxiv.org/abs/1706.03762
    \endverb
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.CL}
    \field{year}{2023}
  \endentry

  \entry{wang2024mambarvisionmambaneeds}{misc}{}{}
    \name{author}{9}{}{%
      {{hash=WF}{%
         family={Wang},
         familyi={W\bibinitperiod},
         given={Feng},
         giveni={F\bibinitperiod},
      }}%
      {{hash=WJ}{%
         family={Wang},
         familyi={W\bibinitperiod},
         given={Jiahao},
         giveni={J\bibinitperiod},
      }}%
      {{hash=RS}{%
         family={Ren},
         familyi={R\bibinitperiod},
         given={Sucheng},
         giveni={S\bibinitperiod},
      }}%
      {{hash=WG}{%
         family={Wei},
         familyi={W\bibinitperiod},
         given={Guoyizhe},
         giveni={G\bibinitperiod},
      }}%
      {{hash=MJ}{%
         family={Mei},
         familyi={M\bibinitperiod},
         given={Jieru},
         giveni={J\bibinitperiod},
      }}%
      {{hash=SW}{%
         family={Shao},
         familyi={S\bibinitperiod},
         given={Wei},
         giveni={W\bibinitperiod},
      }}%
      {{hash=ZY}{%
         family={Zhou},
         familyi={Z\bibinitperiod},
         given={Yuyin},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=YA}{%
         family={Yuille},
         familyi={Y\bibinitperiod},
         given={Alan},
         giveni={A\bibinitperiod},
      }}%
      {{hash=XC}{%
         family={Xie},
         familyi={X\bibinitperiod},
         given={Cihang},
         giveni={C\bibinitperiod},
      }}%
    }
    \strng{namehash}{WF+1}
    \strng{fullhash}{WFWJRSWGMJSWZYYAXC1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{W}
    \field{sortinithash}{W}
    \verb{eprint}
    \verb 2405.14858
    \endverb
    \field{title}{Mamba-R: Vision Mamba ALSO Needs Registers}
    \verb{url}
    \verb https://arxiv.org/abs/2405.14858
    \endverb
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.CV}
    \field{year}{2024}
  \endentry

  \entry{wen2024efficientvisionlanguagemodelssummarizing}{misc}{}{}
    \name{author}{5}{}{%
      {{hash=WY}{%
         family={Wen},
         familyi={W\bibinitperiod},
         given={Yuxin},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=CQ}{%
         family={Cao},
         familyi={C\bibinitperiod},
         given={Qingqing},
         giveni={Q\bibinitperiod},
      }}%
      {{hash=FQ}{%
         family={Fu},
         familyi={F\bibinitperiod},
         given={Qichen},
         giveni={Q\bibinitperiod},
      }}%
      {{hash=MS}{%
         family={Mehta},
         familyi={M\bibinitperiod},
         given={Sachin},
         giveni={S\bibinitperiod},
      }}%
      {{hash=NM}{%
         family={Najibi},
         familyi={N\bibinitperiod},
         given={Mahyar},
         giveni={M\bibinitperiod},
      }}%
    }
    \strng{namehash}{WY+1}
    \strng{fullhash}{WYCQFQMSNM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{W}
    \field{sortinithash}{W}
    \verb{eprint}
    \verb 2410.14072
    \endverb
    \field{title}{Efficient Vision-Language Models by Summarizing Visual Tokens into Compact Registers}
    \verb{url}
    \verb https://arxiv.org/abs/2410.14072
    \endverb
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.CV}
    \field{year}{2024}
  \endentry

  \entry{Yin_2022_CVPR}{inproceedings}{}{}
    \name{author}{6}{}{%
      {{hash=YH}{%
         family={Yin},
         familyi={Y\bibinitperiod},
         given={Hongxu},
         giveni={H\bibinitperiod},
      }}%
      {{hash=VA}{%
         family={Vahdat},
         familyi={V\bibinitperiod},
         given={Arash},
         giveni={A\bibinitperiod},
      }}%
      {{hash=AJM}{%
         family={Alvarez},
         familyi={A\bibinitperiod},
         given={Jose\bibnamedelima M.},
         giveni={J\bibinitperiod\bibinitdelim M\bibinitperiod},
      }}%
      {{hash=MA}{%
         family={Mallya},
         familyi={M\bibinitperiod},
         given={Arun},
         giveni={A\bibinitperiod},
      }}%
      {{hash=KJ}{%
         family={Kautz},
         familyi={K\bibinitperiod},
         given={Jan},
         giveni={J\bibinitperiod},
      }}%
      {{hash=MP}{%
         family={Molchanov},
         familyi={M\bibinitperiod},
         given={Pavlo},
         giveni={P\bibinitperiod},
      }}%
    }
    \strng{namehash}{YH+1}
    \strng{fullhash}{YHVAAJMMAKJMP1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{Y}
    \field{sortinithash}{Y}
    \field{booktitle}{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}
    \field{pages}{10809\bibrangedash 10818}
    \field{title}{A-ViT: Adaptive Tokens for Efficient Vision Transformer}
    \field{year}{2022}
    \warn{\item Invalid format of field 'month'}
  \endentry
\enddatalist
\endinput
