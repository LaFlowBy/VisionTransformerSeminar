@misc{registers,
      title={Vision Transformers Need Registers}, 
      author={Timothée Darcet and Maxime Oquab and Julien Mairal and Piotr Bojanowski},
      year={2024},
      eprint={2309.16588},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2309.16588}, 
}
@misc{wang2024mambarvisionmambaneeds,
  title         = {Mamba-R: Vision Mamba ALSO Needs Registers},
  author        = {Feng Wang and Jiahao Wang and Sucheng Ren and Guoyizhe Wei and Jieru Mei and Wei Shao and Yuyin Zhou and Alan Yuille and Cihang Xie},
  year          = {2024},
  eprint        = {2405.14858},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2405.14858}
}

@article{10.1145/3505244,
  author     = {Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
  title      = {Transformers in Vision: A Survey},
  year       = {2022},
  issue_date = {January 2022},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {54},
  number     = {10s},
  issn       = {0360-0300},
  url        = {https://doi.org/10.1145/3505244},
  doi        = {10.1145/3505244},
  abstract   = {Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks, e.g., Long short-term memory. Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text, and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers, i.e., self-attention, large-scale pre-training, and bidirectional feature encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization), and three-dimensional analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works. We hope this effort will ignite further interest in the community to solve current challenges toward the application of transformer models in computer vision.},
  journal    = {ACM Comput. Surv.},
  month      = sep,
  articleno  = {200},
  numpages   = {41},
  keywords   = {Self-attention, transformers, bidirectional encoders, deep neural networks, convolutional networks, self-supervision, literature survey}
}

@misc{wen2024efficientvisionlanguagemodelssummarizing,
  title         = {Efficient Vision-Language Models by Summarizing Visual Tokens into Compact Registers},
  author        = {Yuxin Wen and Qingqing Cao and Qichen Fu and Sachin Mehta and Mahyar Najibi},
  year          = {2024},
  eprint        = {2410.14072},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2410.14072}
}

@article{10.1145/3586074,
  author     = {Fournier, Quentin and Caron, Ga\'{e}tan Marceau and Aloise, Daniel},
  title      = {A Practical Survey on Faster and Lighter Transformers},
  year       = {2023},
  issue_date = {December 2023},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {55},
  number     = {14s},
  issn       = {0360-0300},
  url        = {https://doi.org/10.1145/3586074},
  doi        = {10.1145/3586074},
  abstract   = {Recurrent neural networks are effective models to process sequences. However, they are unable to learn long-term dependencies because of their inherent sequential nature. As a solution, Vaswani et al. introduced the Transformer, a model solely based on the attention mechanism that is able to relate any two positions of the input sequence, hence modelling arbitrary long dependencies. The Transformer has improved the state-of-the-art across numerous sequence modelling tasks. However, its effectiveness comes at the expense of a quadratic computational and memory complexity with respect to the sequence length, hindering its adoption. Fortunately, the deep learning community has always been interested in improving the models’ efficiency, leading to a plethora of solutions such as parameter sharing, pruning, mixed-precision, and knowledge distillation. Recently, researchers have directly addressed the Transformer’s limitation by designing lower-complexity alternatives such as the Longformer, Reformer, Linformer, and Performer. However, due to the wide range of solutions, it has become challenging for researchers and practitioners to determine which methods to apply in practice to meet the desired tradeoff between capacity, computation, and memory. This survey addresses this issue by investigating popular approaches to make Transformers faster and lighter and by providing a comprehensive explanation of the methods’ strengths, limitations, and underlying assumptions.},
  journal    = {ACM Comput. Surv.},
  month      = jul,
  articleno  = {304},
  numpages   = {40},
  keywords   = {survey, self-attention, efficient transformer, Deep learning}
}

@inproceedings{10.1007/978-3-031-20053-3_30,
  author    = {Touvron, Hugo
               and Cord, Matthieu
               and J{\'e}gou, Herv{\'e}},
  editor    = {Avidan, Shai
               and Brostow, Gabriel
               and Ciss{\'e}, Moustapha
               and Farinella, Giovanni Maria
               and Hassner, Tal},
  title     = {DeiT III: Revenge of the ViT},
  booktitle = {Computer Vision -- ECCV 2022},
  year      = {2022},
  publisher = {Springer Nature Switzerland},
  address   = {Cham},
  pages     = {516--533},
  abstract  = {A Vision Transformer (ViT) is a simple neural architecture amenable to serve several computer vision tasks. It has limited built-in architectural priors, in contrast to more recent architectures that incorporate priors either about the input data or of specific tasks. Recent works show that ViTs benefit from self-supervised pre-training, in particular BerT-like pre-training like BeiT.},
  isbn      = {978-3-031-20053-3}
}

@inproceedings{Yin_2022_CVPR,
  author    = {Yin, Hongxu and Vahdat, Arash and Alvarez, Jose M. and Mallya, Arun and Kautz, Jan and Molchanov, Pavlo},
  title     = {A-ViT: Adaptive Tokens for Efficient Vision Transformer},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2022},
  pages     = {10809-10818}
}

@misc{ryoo2022tokenlearner8learnedtokens,
  title         = {TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?},
  author        = {Michael S. Ryoo and AJ Piergiovanni and Anurag Arnab and Mostafa Dehghani and Anelia Angelova},
  year          = {2022},
  eprint        = {2106.11297},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2106.11297}
}

@misc{vit-state-challenges,
  title         = {Vision Transformers: State of the Art and Research Challenges},
  author        = {Bo-Kai Ruan and Hong-Han Shuai and Wen-Huang Cheng},
  year          = {2022},
  eprint        = {2207.03041},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2207.03041}
}

@article{Liu2024-lm,
  title    = {Vision transformers with hierarchical attention},
  author   = {Liu, Yun and Wu, Yu-Huan and Sun, Guolei and Zhang, Le and
              Chhatkuli, Ajad and Van Gool, Luc},
  journal  = {Mach. Intell. Res.},
  volume   = 21,
  number   = 4,
  pages    = {670--683},
  year     = 2024,
  language = {en}
}

@misc{visiontransformers2021,
  title         = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author        = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
  year          = {2021},
  eprint        = {2010.11929},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2010.11929}
}

@misc{transformer2017,
  title         = {Attention Is All You Need},
  author        = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  year          = {2023},
  eprint        = {1706.03762},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/1706.03762}
}

@misc{dino,
      title={Emerging Properties in Self-Supervised Vision Transformers}, 
      author={Mathilde Caron and Hugo Touvron and Ishan Misra and Hervé Jégou and Julien Mairal and Piotr Bojanowski and Armand Joulin},
      year={2021},
      eprint={2104.14294},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2104.14294}, 
}

@misc{dinov2,
      title={DINOv2: Learning Robust Visual Features without Supervision}, 
      author={Maxime Oquab and Timothée Darcet and Théo Moutakanni and Huy Vo and Marc Szafraniec and Vasil Khalidov and Pierre Fernandez and Daniel Haziza and Francisco Massa and Alaaeldin El-Nouby and Mahmoud Assran and Nicolas Ballas and Wojciech Galuba and Russell Howes and Po-Yao Huang and Shang-Wen Li and Ishan Misra and Michael Rabbat and Vasu Sharma and Gabriel Synnaeve and Hu Xu and Hervé Jegou and Julien Mairal and Patrick Labatut and Armand Joulin and Piotr Bojanowski},
      year={2024},
      eprint={2304.07193},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2304.07193}, 
}

@misc{dino,
      title={Emerging Properties in Self-Supervised Vision Transformers}, 
      author={Mathilde Caron and Hugo Touvron and Ishan Misra and Hervé Jégou and Julien Mairal and Piotr Bojanowski and Armand Joulin},
      year={2021},
      eprint={2104.14294},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2104.14294}, 
}

@misc{dinov2,
      title={DINOv2: Learning Robust Visual Features without Supervision}, 
      author={Maxime Oquab and Timothée Darcet and Théo Moutakanni and Huy Vo and Marc Szafraniec and Vasil Khalidov and Pierre Fernandez and Daniel Haziza and Francisco Massa and Alaaeldin El-Nouby and Mahmoud Assran and Nicolas Ballas and Wojciech Galuba and Russell Howes and Po-Yao Huang and Shang-Wen Li and Ishan Misra and Michael Rabbat and Vasu Sharma and Gabriel Synnaeve and Hu Xu and Hervé Jegou and Julien Mairal and Patrick Labatut and Armand Joulin and Piotr Bojanowski},
      year={2024},
      eprint={2304.07193},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2304.07193}, 
}